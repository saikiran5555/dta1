{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f099dc1",
   "metadata": {},
   "source": [
    "A confusion matrix is a table used in machine learning classification to evaluate the performance of a model by comparing the actual and predicted classes. It provides a comprehensive view of the model's predictions, breaking down the results into four categories: true positive (TP), true negative (TN), false positive (FP), and false negative (FN)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cd15f2",
   "metadata": {},
   "source": [
    "                Predicted\n",
    "             |  Positive | Negative |\n",
    "-------------------------------------\n",
    "Actual  Positive | TP        | FN       |\n",
    "        Negative | FP        | TN       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd93aaf",
   "metadata": {},
   "source": [
    "rue Positive (TP): Instances where the model correctly predicted the positive class.\n",
    "True Negative (TN): Instances where the model correctly predicted the negative class.\n",
    "False Positive (FP): Instances where the model predicted the positive class incorrectly.\n",
    "False Negative (FN): Instances where the model predicted the negative class incorrectly.\n",
    "Now, let's define precision, recall, and F1 score based on these elements:\n",
    "\n",
    "Precision:\n",
    "\n",
    "Precision is the ratio of true positives to the sum of true positives and false positives.\n",
    "Precision measures the accuracy of the positive predictions made by the model.\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "=\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "�\n",
    "Precision= \n",
    "TP+FP\n",
    "TP\n",
    "​\n",
    " \n",
    "\n",
    "Recall (Sensitivity or True Positive Rate):\n",
    "\n",
    "Recall is the ratio of true positives to the sum of true positives and false negatives.\n",
    "Recall measures the ability of the model to capture all the positive instances.\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "=\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "�\n",
    "Recall= \n",
    "TP+FN\n",
    "TP\n",
    "​\n",
    " \n",
    "\n",
    "F1 Score:\n",
    "\n",
    "The F1 score is the harmonic mean of precision and recall. It provides a balance between precision and recall.\n",
    "It is particularly useful when there is an uneven class distribution.\n",
    "�\n",
    "1\n",
    "=\n",
    "2\n",
    "×\n",
    "Precision\n",
    "×\n",
    "Recall\n",
    "Precision\n",
    "+\n",
    "Recall\n",
    "F1= \n",
    "Precision+Recall\n",
    "2×Precision×Recall\n",
    "​\n",
    " \n",
    "\n",
    "These metrics are valuable for evaluating different aspects of a model's performance:\n",
    "\n",
    "Precision: High precision indicates that when the model predicts the positive class, it is likely to be correct.\n",
    "Recall: High recall suggests that the model captures most of the actual positive instances.\n",
    "F1 Score: Balances precision and recall; it is useful when there is a need for a trade-off between false positives and false negatives."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
